{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t \n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from torchvision import datasets, transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' # cuda or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "init_log_sigma = -3.0 # логарифм дисперсии вариационного распределения при инициализации\n",
    "prior_sigma = 0.1 # априорная дисперсия\n",
    "epoch_num = 50 #количество эпох\n",
    "lamb = [0, 0.1, 0.5, 1, 5, 10, 100, 1000]\n",
    "# lam = 1.0 # коэффициент перед дивергенцией\n",
    "hidden_num = 100 # количество нейронов на скрытом слое\n",
    "t.manual_seed(42) # задаем значение генератора случайных чисел для повторяемости экспериментов\n",
    "acc_delete = [] \n",
    "filename = 'Hypernet_lowrank_lamhn1' # куда сохранять\n",
    "lam_hidden_num = 1\n",
    "start_num = 1\n",
    "log_lam_low = -2.0\n",
    "log_lam_high = 2.0\n",
    "warmup_epochs = 3 \n",
    "mode = 'lowrank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение данных\n",
    "def save(file):\n",
    "    outfile = open(filename, 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "def load(path = filename):\n",
    "    infile = open(path, 'rb')\n",
    "    file = pickle.load(infile)\n",
    "    infile.close()\n",
    "    return file\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных\n",
    "train_data = torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                             transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                              ]))\n",
    "\n",
    "test_data = torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                             transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                              ]))\n",
    "\n",
    "\n",
    "train_loader = t.utils.data.DataLoader(train_data, batch_size=batch_size, pin_memory=True )\n",
    "test_loader = t.utils.data.DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankNet(nn.Module):\n",
    "    def __init__(self, size, hidden, gain_const = 1.0, gain_lamb = 1.0,\n",
    "                 gain_lowrank = .0001,  act= lambda x: x):    \n",
    "        nn.Module.__init__(self)        \n",
    "        self.w = nn.Linear(1, hidden).to(device)\n",
    "        t.nn.init.xavier_uniform(self.w.weight, gain_lamb)\n",
    "        # проверка на вектор или матрица\n",
    "        if isinstance(size, tuple) and len(size) == 2: # если сайз неизменяемый список и его длина 2\n",
    "            self.in_, self.out_ = size\n",
    "            self.diagonal = False\n",
    "        else:\n",
    "            self.out_ = size\n",
    "            self.diagonal = True\n",
    "            \n",
    "        \n",
    "        self.one = t.ones(1,device=device) # для упрощения работы с лямбдой. Костыль, можно сделать проще\n",
    "        self.act = act\n",
    "        \n",
    "        if self.diagonal:\n",
    "            self.w_d = nn.Linear(hidden, self.out_).to(device)\n",
    "            t.nn.init.xavier_uniform(self.w_d.weight, gain_lowrank)\n",
    "            # независимая от параметра lambda часть\n",
    "            self.const = nn.Parameter(t.randn(self.out_, device=device)) \n",
    "            \n",
    "        else:\n",
    "            self.w_a1 = nn.Linear(hidden, self.in_).to(device)\n",
    "            t.nn.init.xavier_uniform(self.w_a1.weight, gain_lowrank)\n",
    "            \n",
    "            self.w_a2 = nn.Linear(hidden, self.out_).to(device)\n",
    "            t.nn.init.xavier_uniform(self.w_a2.weight, gain_lowrank)\n",
    "            \n",
    "            self.w_b = nn.Linear(hidden, self.out_).to(device)\n",
    "            t.nn.init.xavier_uniform(self.w_b.weight, gain_lowrank)\n",
    "            \n",
    "            self.const = nn.Parameter(t.randn(self.in_, self.out_, device=device)) \n",
    "            t.nn.init.xavier_uniform(self.const,  gain_const)\n",
    "            \n",
    "            \n",
    "    def forward(self, lam):\n",
    "        h = self.act(self.w(self.one * lam))        \n",
    "        if self.diagonal:\n",
    "            return self.const + self.w_d(h)\n",
    "        else:\n",
    "            a1 = self.w_a1(h)\n",
    "            a2 = self.w_a2(h)\n",
    "            b = self.w_b(h)\n",
    "         \n",
    "            return self.const +  t.matmul(a1.view(-1, 1), a2.view(1, -1)) + b\n",
    "\n",
    "        \n",
    "class LinearApprNet(nn.Module):\n",
    "    def __init__(self, size,  gain_const = 1.0, gain_const2 = 0.000001,  act= lambda x: x):    \n",
    "        nn.Module.__init__(self)        \n",
    "        if isinstance(size, tuple) and len(size) == 2:\n",
    "            self.in_, self.out_ = size\n",
    "            self.diagonal = False\n",
    "        else:\n",
    "            self.out_ = size\n",
    "            self.diagonal = True\n",
    "            \n",
    "        \n",
    "        self.one = t.ones(1,device=device) # для упрощения работы с лямбдой. Костыль, можно сделать проще\n",
    "        self.act = act\n",
    "        \n",
    "        if self.diagonal:\n",
    "            # независимая от параметра lambda часть\n",
    "            self.const = nn.Parameter(t.randn(self.out_, device=device)) \n",
    "            self.const2 = nn.Parameter(t.ones(self.out_, device=device) * gain_const2) \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            self.const = nn.Parameter(t.randn(self.in_, self.out_, device=device)) \n",
    "            t.nn.init.xavier_uniform(self.const,  gain_const)\n",
    "            self.const2 = nn.Parameter(t.randn(self.in_, self.out_, device=device)) \n",
    "            t.nn.init.xavier_uniform(self.const2,  gain_const2)\n",
    "            \n",
    "            \n",
    "    def forward(self, lam):        \n",
    "        if self.diagonal:\n",
    "            return self.const + self.const2 * lam\n",
    "        else:\n",
    "            return self.const + self.const2 * lam \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:21: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2171e-03,  2.7072e-04, -1.7269e-03, -7.0226e-04, -5.9742e-04,\n",
       "         2.1653e-03, -4.3578e-04,  9.9391e-04, -3.8922e-04,  2.2368e-03,\n",
       "         1.0792e-03, -7.2050e-04,  1.8463e-03,  3.1313e-03,  3.4079e-03,\n",
       "        -9.5746e-04, -2.6060e-03, -9.3341e-05,  9.2231e-04, -3.2520e-04,\n",
       "        -1.9574e-03, -1.0403e-03, -1.1552e-03, -1.9388e-03, -6.0648e-04,\n",
       "         9.6023e-05,  1.5191e-03, -9.0298e-04,  1.2062e-04, -3.1674e-04,\n",
       "        -6.9535e-04,  8.0307e-04,  1.7796e-03,  1.4316e-03, -9.2125e-04,\n",
       "        -1.3005e-03,  2.3808e-03, -2.3137e-04, -2.2714e-03,  5.8860e-04,\n",
       "        -7.6669e-04, -3.7073e-04, -4.5758e-04,  1.0692e-03,  1.1842e-03,\n",
       "        -1.2034e-04, -9.3532e-04,  4.8220e-05, -2.8507e-03,  2.6738e-03,\n",
       "        -1.5985e-03,  6.3312e-04,  1.6755e-03,  1.2929e-03,  4.1199e-04,\n",
       "         4.4294e-04,  4.3845e-04, -6.0153e-04,  2.2393e-03,  1.4299e-03,\n",
       "         2.7205e-03, -3.5923e-03, -2.3419e-04, -3.5461e-03, -5.7304e-04,\n",
       "        -3.5763e-04, -7.5352e-04,  6.7401e-04,  2.1608e-04, -1.8482e-03,\n",
       "         1.3292e-03, -1.3154e-03,  9.8175e-04,  2.0151e-03,  1.0116e-03,\n",
       "         4.4405e-05,  3.3430e-03, -2.0286e-03,  3.2514e-04,  3.5479e-03,\n",
       "         1.7042e-03,  1.3351e-03, -4.2975e-05, -1.0967e-05,  1.0651e-03,\n",
       "        -2.1803e-03, -2.7871e-04,  3.1860e-03,  3.9499e-03, -1.5126e-03,\n",
       "         1.1978e-03, -1.4479e-03, -1.3227e-03,  2.0039e-03, -1.5665e-03,\n",
       "        -4.4121e-03,  2.4953e-03, -1.0599e-04,  2.1583e-03, -1.2161e-03],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка что все работает\n",
    "# случай вектора\n",
    "n = LowRankNet(100, 10)\n",
    "n(100)- n(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:27: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:36: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0018, -0.0042, -0.0005,  ..., -0.0009, -0.0017,  0.0036],\n",
       "        [-0.0010, -0.0064, -0.0013,  ..., -0.0002, -0.0046,  0.0019],\n",
       "        [-0.0017, -0.0045, -0.0008,  ..., -0.0013, -0.0023,  0.0039],\n",
       "        ...,\n",
       "        [-0.0017, -0.0044, -0.0009,  ..., -0.0017, -0.0023,  0.0045],\n",
       "        [-0.0011, -0.0064, -0.0011,  ...,  0.0002, -0.0044,  0.0014],\n",
       "        [-0.0010, -0.0068, -0.0011,  ...,  0.0006, -0.0047,  0.0008]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка что все работает\n",
    "# случай матрицы\n",
    "n = LowRankNet((100, 20), 10)\n",
    "n(100) - n(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarLayer(nn.Module): # вариационная однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = nn.Parameter(t.randn(in_, out_, device=device)) # параметры средних\n",
    "        t.nn.init.xavier_uniform(self.mean) \n",
    "        self.log_sigma = nn.Parameter(t.ones(in_, out_, device = device)*init_log_sigma) # логарифм дисперсии\n",
    "        self.mean_b = nn.Parameter(t.randn(out_, device=device)) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = nn.Parameter(t.ones(out_, device=device) * init_log_sigma)\n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.training: # во время обучения - сэмплируем из нормального распределения\n",
    "            self.eps_w = t.distributions.Normal(self.mean, t.exp(self.log_sigma))\n",
    "            self.eps_b = t.distributions.Normal(self.mean_b, t.exp(self.log_sigma_b))\n",
    "        \n",
    "            w = self.eps_w.rsample()\n",
    "            b = self.eps_b.rsample()\n",
    "             \n",
    "        else:  # во время контроля - смотрим средние значения параметра        \n",
    "            w = self.mean \n",
    "            b = self.mean_b\n",
    "            \n",
    "        # функция активации \n",
    "        return self.act(t.matmul(x, w)+b)\n",
    "\n",
    "    def KLD(self):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean, t.exp(self.log_sigma))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b,  t.exp(self.log_sigma_b))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2\n",
    "    \n",
    "class VarLayerLowRank(nn.Module): # вариационная однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = LowRankNet((in_, out_), lam_hidden_num) # параметры средних            \n",
    "        self.log_sigma = LowRankNet((in_, out_), lam_hidden_num) # логарифм дисперсии\n",
    "        self.mean_b = LowRankNet( out_, lam_hidden_num) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = LowRankNet( out_, lam_hidden_num)\n",
    "     \n",
    "        self.log_sigma.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma.const.data+= init_log_sigma\n",
    "     \n",
    "        self.log_sigma_b.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma_b.const.data+= init_log_sigma\n",
    "        \n",
    "        \n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,x, l):\n",
    "        if self.training: # во время обучения - сэмплируем из нормального распределения\n",
    "            self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "            self.eps_b = t.distributions.Normal(self.mean_b(l), t.exp(self.log_sigma_b(l)))\n",
    "        \n",
    "            w = self.eps_w.rsample()\n",
    "            b = self.eps_b.rsample()\n",
    "             \n",
    "        else:  # во время контроля - смотрим средние значения параметра        \n",
    "            w = self.mean(l) \n",
    "            b = self.mean_b(l)\n",
    "            \n",
    "        # функция активации \n",
    "        return self.act(t.matmul(x, w)+b)\n",
    "\n",
    "    def KLD(self, l):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b(l),  t.exp(self.log_sigma_b(l)))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2\n",
    "    \n",
    "\n",
    "class VarLayerLinearAppr(nn.Module): # вариационная однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = LinearApprNet((in_, out_)) # параметры средних            \n",
    "        self.log_sigma = LinearApprNet((in_, out_)) # логарифм дисперсии\n",
    "        self.mean_b = LinearApprNet( out_) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = LinearApprNet( out_)\n",
    "     \n",
    "        self.log_sigma.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma.const.data+= init_log_sigma\n",
    "     \n",
    "        self.log_sigma_b.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma_b.const.data+= init_log_sigma\n",
    "        \n",
    "        \n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,x, l):\n",
    "        if self.training: # во время обучения - сэмплируем из нормального распределения\n",
    "            self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "            self.eps_b = t.distributions.Normal(self.mean_b(l), t.exp(self.log_sigma_b(l)))\n",
    "        \n",
    "            w = self.eps_w.rsample()\n",
    "            b = self.eps_b.rsample()\n",
    "             \n",
    "        else:  # во время контроля - смотрим средние значения параметра        \n",
    "            w = self.mean(l) \n",
    "            b = self.mean_b(l)\n",
    "            \n",
    "        # функция активации \n",
    "        return self.act(t.matmul(x, w)+b)\n",
    "\n",
    "    def KLD(self, l):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b(l),  t.exp(self.log_sigma_b(l)))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:73: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "l = VarLayerLinearAppr(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.9999, -2.9999, -2.9999, -2.9999, -2.9999, -2.9999, -2.9999, -2.9999,\n",
       "        -2.9999, -2.9999], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.log_sigma_b(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarSeqNet(nn.Sequential):    \n",
    "    # класс-обертка на случай, если у нас многослойная нейронная сеть\n",
    "    def KLD(self, lam = None):\n",
    "        k = 0\n",
    "        for l in self: \n",
    "            if lam is None:\n",
    "                k+=l.KLD()\n",
    "            else:\n",
    "                k+=l.KLD(lam)\n",
    "                \n",
    "        return k\n",
    "    \n",
    "    def forward(self, x, lam = None):\n",
    "        if lam is None:\n",
    "            for l in self:\n",
    "                x = l(x)\n",
    "            return x\n",
    "        else:\n",
    "            for l in self:\n",
    "                x = l(x, lam)\n",
    "            return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batches(net, loss_fn, optimizer, i, out, out_loss, kld, loss, epoch):\n",
    "    for id, (x,y) in enumerate(train_loader):  \n",
    "            id+=1\n",
    "            if device == 'cuda':\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()            \n",
    "            optimizer.zero_grad() \n",
    "            loss[i] = 0 \n",
    "            #for _ in range(5):\n",
    "            log_lam = np.random.uniform(low=log_lam_low, high=log_lam_high)\n",
    "            lam = 10**log_lam \n",
    "            #lam = 1.0\n",
    "            lam_param = lam/10**(log_lam_high) # нормируем вход\n",
    "            out[i] = net(x, lam_param)\n",
    "            # правдоподобие должно суммироваться по всей обучающей выборке\n",
    "            # в случае батчей - она приводится к тому же порядку \n",
    "            out_loss[i] = loss_fn(out[i], y)* len(train_data) \n",
    "            #if epoch > warmup_epochs:                \n",
    "            kld[i] =  net.KLD( lam_param) *lam\n",
    "            #else:\n",
    "            #    kld[i] =  net.KLD(lam_param) * 0.0\n",
    "\n",
    "\n",
    "            loss[i] += (out_loss[i]+kld[i])       \n",
    "            if id %100 == 0:           \n",
    "                print (\"Number of net:\",i, loss[i].data, out_loss[i].data, kld[i].data, lam)            \n",
    "                    \n",
    "            loss[i].backward()       \n",
    "            clip_grad_value_(net.parameters(), 1.0) # для стабильности градиента. С этим можно играться\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(net, loss_fn, i, kld, loss, out, out_loss):\n",
    "    net.eval()  \n",
    "    kld[i] =  net.KLD(1) \n",
    "    loss[i] = kld[i]\n",
    "    for x,y in test_loader:\n",
    "         if device == 'cuda':\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()          \n",
    "    out[i] = net(x, 1)   \n",
    "    out_loss[i] = loss_fn(out[i], y)* len(train_data)\n",
    "    #  print(out_loss[i])\n",
    "    # print(loss[i])\n",
    "    loss[i] += out_loss[i]\n",
    "    net.train()\n",
    "    print (loss[i])\n",
    "    return loss[i]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# рассмотрим для примера сеть, состояющую из двух слоев\n",
    "# второй слой - softmax. По сути для обучения задавать активацию явно не нужно, она забита в nn.CrossEntropyLoss\n",
    "def init_nets(loss_fn_nets):\n",
    "    for i in range(start_num):\n",
    "        if mode == 'lowrank':\n",
    "            nets.append(VarSeqNet(VarLayerLowRank(784,  hidden_num), VarLayerLowRank(hidden_num, 10, act=lambda x:x)))\n",
    "        elif mode == 'linear':\n",
    "            nets.append(VarSeqNet(VarLayerLinearAppr(784,  hidden_num), VarLayerLinearAppr(hidden_num, 10, act=lambda x:x)))\n",
    "        else:\n",
    "            raise ValueError('Bad mode')\n",
    "        optimizer_nets.append(optim.Adam(nets[i].parameters(), lr=0.001))\n",
    "        loss_fn_nets.append(nn.CrossEntropyLoss())\n",
    "    loss_graph=[[],[],[]]\n",
    "    out = [None, None, None]\n",
    "    out_loss = [None, None, None]\n",
    "    kld = [None, None, None]\n",
    "    loss = [None, None, None]\n",
    "    return out, out_loss, kld, loss, loss_graph\n",
    "\n",
    "def train_nets(out, out_loss, kld, loss, loss_graph):\n",
    "    for epoch in range(epoch_num):             \n",
    "        for i,net in enumerate(nets):\n",
    "            train_batches(net,loss_fn_nets[i], optimizer_nets[i],i, out, out_loss, kld, loss, epoch)\n",
    "        print ('end of epoch: ', epoch)   \n",
    "        for i,net in enumerate(nets):\n",
    "            print(\"Number of net:\",i)        \n",
    "            loss_graph[i].append(statistic(net, loss_fn_nets[i], i, kld, loss, out, out_loss))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(loss_graf)\n",
    "def graph_loss_func(loss_graph, nets):\n",
    "    for i,net in enumerate(nets): \n",
    "        plt.plot(loss_graph[i])\n",
    "    plt.ylabel('Loss function')\n",
    "    plt.xlabel('Number of epoche')\n",
    "    plt.show()\n",
    "#print(out_loss)\n",
    "\n",
    "#graph_loss_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(out): # точность классификации\n",
    "    acc = []\n",
    "    for i,net in enumerate(nets):\n",
    "        correct = 0\n",
    "        net.eval()\n",
    "        for x,y in test_loader:\n",
    "            if device == 'cuda':\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()     \n",
    "            out[i] = net(x)    \n",
    "            correct += out[i].argmax(1).eq(y).sum().cpu().numpy()\n",
    "        acc.append(correct / len(test_data))\n",
    "    print(sum(acc)/len(acc))   \n",
    "    return(acc)\n",
    "#test_acc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# коэффициенты информативности, см. статью practical variational inference\n",
    "# попробуем удалять параметры первого слоя по этому коэффициенту\n",
    "\n",
    "def init_coeff(prune_coef, mu, sigma):\n",
    "    for i,net in enumerate(nets): \n",
    "        mu.append(net[0].mean) \n",
    "        sigma.append(t.exp(2*net[0].log_sigma))\n",
    "        prune_coef.append((mu[i]**2/sigma[i]).cpu().detach().numpy())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем удалять по 10% от модели и смотреть качество\n",
    "def delete_10(acc_delete, prune_coef, mu, sigma, nets, out):\n",
    "    acc_delete = []\n",
    "    sorted_coefs = []\n",
    "    for i, net in enumerate(nets):\n",
    "        sorted_coefs.append(np.sort(prune_coef[i].flatten()))\n",
    "    for j in range(10):\n",
    "        for i,net in enumerate(nets): \n",
    "            ids = (prune_coef[i] <= sorted_coefs[i][round(j/10*len(sorted_coefs[i]))]) \n",
    "            net[0].mean.data*=(1-t.tensor(ids*1.0, device=device, dtype=t.float))\n",
    "            print ('nonzero params: ', (abs(net[0].mean)>0).float().mean())\n",
    "        acc_delete.append(test_acc(out))\n",
    "    return acc_delete    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(acc_delete, lamb):\n",
    "    proc = [0,10,20,30,40,50,60,70,80,90]\n",
    "    plt.rcParams['figure.figsize'] = 12, 12\n",
    "    for k, lam in enumerate(lamb):\n",
    "        acc_delete_n = np.array(acc_delete[k])\n",
    "        plt.plot(proc, np.mean(acc_delete_n, 1), label = 'lambda = {}'.format(str(lam)))\n",
    "        # откладываем ошибку вокруг среднего, альфа - прозрачность линии\n",
    "        plt.fill_between(proc, np.mean(acc_delete_n, 1)  + np.std(acc_delete_n, 1) , np.mean(acc_delete_n, 1) - np.std(acc_delete_n, 1) , alpha = 0.5 )\n",
    "    plt.ylabel('Точность классификации', fontsize = 20)\n",
    "    plt.xlabel('Процент удаления', fontsize = 20)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('1')\n",
    "    plt.show()\n",
    "\n",
    "#acc_delete = load('save_array_0.1')    \n",
    "#graph(acc_delete, lamb)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем, что фокусов тут нет, удаляем оставшиеся 10%\\\n",
    "def delete_last10():\n",
    "    flag = 0\n",
    "    for j in range(10):\n",
    "        for i,net in enumerate(nets): \n",
    "            if (flag == 0):\n",
    "                sorted_coefs = np.sort(prune_coef[i].flatten())\n",
    "                flag = 1\n",
    "            ids = (prune_coef[i] <= sorted_coefs[round((0.9+j/100)*len(sorted_coefs))]) \n",
    "            net[0].mean.data*=(1-t.tensor(ids*1.0, device=device, dtype=t.float))\n",
    "            print ('nonzero params: ', (abs(net[0].mean)>0).float().mean())\n",
    "        (test_acc())\n",
    "    for i,net in enumerate(nets):\n",
    "        net[0].mean.data*=0\n",
    "        print ('nonzero params: ', (abs(net[0].mean)>0).float().mean())\n",
    "    (test_acc())\n",
    "    \n",
    "#delete_last10()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:27: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:36: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:21: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of net: 0 tensor(1.6137e+08, device='cuda:0') tensor(1.3356e+08, device='cuda:0') tensor(27804318., device='cuda:0') 32.77495446971793\n",
      "Number of net: 0 tensor(55984864., device='cuda:0') tensor(54412196., device='cuda:0') tensor(1572666.7500, device='cuda:0') 3.4568149074219034\n",
      "Number of net: 0 tensor(21751420., device='cuda:0') tensor(19422400., device='cuda:0') tensor(2329019., device='cuda:0') 8.58096363149618\n",
      "Number of net: 0 tensor(8824769., device='cuda:0') tensor(5582224.5000, device='cuda:0') tensor(3242544.5000, device='cuda:0') 16.319677612708162\n",
      "Number of net: 0 tensor(5704550., device='cuda:0') tensor(4189776.2500, device='cuda:0') tensor(1514774., device='cuda:0') 8.661383336424572\n",
      "Number of net: 0 tensor(4099642.5000, device='cuda:0') tensor(3925222.7500, device='cuda:0') tensor(174419.6406, device='cuda:0') 1.1088039190546484\n",
      "Number of net: 0 tensor(4153422.2500, device='cuda:0') tensor(4095002.7500, device='cuda:0') tensor(58419.4648, device='cuda:0') 0.3892096233040432\n",
      "Number of net: 0 tensor(1547562.5000, device='cuda:0') tensor(1029329., device='cuda:0') tensor(518233.5625, device='cuda:0') 3.393372736977647\n",
      "Number of net: 0 tensor(7316574., device='cuda:0') tensor(2760797., device='cuda:0') tensor(4555777., device='cuda:0') 26.14203787961365\n",
      "end of epoch:  0\n",
      "Number of net: 0\n",
      "tensor(64072680., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(4319443., device='cuda:0') tensor(965387.1875, device='cuda:0') tensor(3354056., device='cuda:0') 19.363430145853165\n",
      "Number of net: 0 tensor(1216493.2500, device='cuda:0') tensor(1126187.2500, device='cuda:0') tensor(90305.9375, device='cuda:0') 0.5466548110678922\n",
      "Number of net: 0 tensor(4587267., device='cuda:0') tensor(2683812.7500, device='cuda:0') tensor(1903454.5000, device='cuda:0') 10.738692159234988\n",
      "Number of net: 0 tensor(42665668., device='cuda:0') tensor(14981071., device='cuda:0') tensor(27684596., device='cuda:0') 73.13287285475931\n",
      "Number of net: 0 tensor(2670188.7500, device='cuda:0') tensor(2620098.2500, device='cuda:0') tensor(50090.5195, device='cuda:0') 0.27987606312691615\n",
      "Number of net: 0 tensor(1565380.7500, device='cuda:0') tensor(811551.8125, device='cuda:0') tensor(753828.8750, device='cuda:0') 3.9631812418348824\n",
      "Number of net: 0 tensor(1205626.5000, device='cuda:0') tensor(1201677.7500, device='cuda:0') tensor(3948.7395, device='cuda:0') 0.02014617108616023\n",
      "Number of net: 0 tensor(1628940.7500, device='cuda:0') tensor(1546445.1250, device='cuda:0') tensor(82495.6172, device='cuda:0') 0.39735033569551237\n",
      "Number of net: 0 tensor(1217624.5000, device='cuda:0') tensor(292816.7812, device='cuda:0') tensor(924807.7500, device='cuda:0') 4.260359659272172\n",
      "end of epoch:  1\n",
      "Number of net: 0\n",
      "tensor(15414096., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(49644064., device='cuda:0') tensor(6650710., device='cuda:0') tensor(42993352., device='cuda:0') 81.8245523237496\n",
      "Number of net: 0 tensor(1220976.6250, device='cuda:0') tensor(1193319.3750, device='cuda:0') tensor(27657.2207, device='cuda:0') 0.11680003795102539\n",
      "Number of net: 0 tensor(737498.6250, device='cuda:0') tensor(618730.3125, device='cuda:0') tensor(118768.3047, device='cuda:0') 0.49200284335025096\n",
      "Number of net: 0 tensor(1476495.7500, device='cuda:0') tensor(1385902.3750, device='cuda:0') tensor(90593.4375, device='cuda:0') 0.36722177517376636\n",
      "Number of net: 0 tensor(2340640., device='cuda:0') tensor(348950.0312, device='cuda:0') tensor(1991689.8750, device='cuda:0') 7.635297987678208\n",
      "Number of net: 0 tensor(9537630., device='cuda:0') tensor(870579.5000, device='cuda:0') tensor(8667051., device='cuda:0') 28.33962672669363\n",
      "Number of net: 0 tensor(425154.3438, device='cuda:0') tensor(344303.2812, device='cuda:0') tensor(80851.0703, device='cuda:0') 0.31674870278675255\n",
      "Number of net: 0 tensor(917173.3125, device='cuda:0') tensor(839760.1250, device='cuda:0') tensor(77413.1719, device='cuda:0') 0.298311522841313\n",
      "Number of net: 0 tensor(319108.7188, device='cuda:0') tensor(304899., device='cuda:0') tensor(14209.7090, device='cuda:0') 0.055158866669691824\n",
      "end of epoch:  2\n",
      "Number of net: 0\n",
      "tensor(16816288., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(1311557.7500, device='cuda:0') tensor(1060768.3750, device='cuda:0') tensor(250789.3438, device='cuda:0') 0.9668764546937595\n",
      "Number of net: 0 tensor(467564.1562, device='cuda:0') tensor(447874.1562, device='cuda:0') tensor(19690.0039, device='cuda:0') 0.07039002220503944\n",
      "Number of net: 0 tensor(216035.3906, device='cuda:0') tensor(207958.3750, device='cuda:0') tensor(8077.0146, device='cuda:0') 0.028772145200760692\n",
      "Number of net: 0 tensor(1998719., device='cuda:0') tensor(389803.1562, device='cuda:0') tensor(1608915.8750, device='cuda:0') 5.561035806045458\n",
      "Number of net: 0 tensor(18407654., device='cuda:0') tensor(3257403.2500, device='cuda:0') tensor(15150250., device='cuda:0') 37.778975802223414\n",
      "Number of net: 0 tensor(7895856., device='cuda:0') tensor(648272.1250, device='cuda:0') tensor(7247584., device='cuda:0') 21.22029765954327\n",
      "Number of net: 0 tensor(263564.5938, device='cuda:0') tensor(174696.3438, device='cuda:0') tensor(88868.2500, device='cuda:0') 0.3022885696228077\n",
      "Number of net: 0 tensor(2436059.5000, device='cuda:0') tensor(542796., device='cuda:0') tensor(1893263.3750, device='cuda:0') 6.265052616755299\n",
      "Number of net: 0 tensor(206293.0938, device='cuda:0') tensor(199744.2656, device='cuda:0') tensor(6548.8311, device='cuda:0') 0.02279706376114126\n",
      "end of epoch:  3\n",
      "Number of net: 0\n",
      "tensor(31578092., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(98165.9844, device='cuda:0') tensor(63354.4922, device='cuda:0') tensor(34811.4883, device='cuda:0') 0.11570076722037045\n",
      "Number of net: 0 tensor(569223.6875, device='cuda:0') tensor(564840.0625, device='cuda:0') tensor(4383.6265, device='cuda:0') 0.014461293471202995\n",
      "Number of net: 0 tensor(2195162.5000, device='cuda:0') tensor(2163224.5000, device='cuda:0') tensor(31938.0664, device='cuda:0') 0.10187587417941993\n",
      "Number of net: 0 tensor(5102206.5000, device='cuda:0') tensor(1001165.7500, device='cuda:0') tensor(4101040.7500, device='cuda:0') 11.989771268856169\n",
      "Number of net: 0 tensor(3752624.5000, device='cuda:0') tensor(446179.5000, device='cuda:0') tensor(3306445., device='cuda:0') 10.135745982704798\n",
      "Number of net: 0 tensor(4725058., device='cuda:0') tensor(345717.7812, device='cuda:0') tensor(4379340., device='cuda:0') 13.306791529770544\n",
      "Number of net: 0 tensor(229546.6250, device='cuda:0') tensor(196617.7344, device='cuda:0') tensor(32928.8828, device='cuda:0') 0.10809199653140579\n",
      "Number of net: 0 tensor(398723.5312, device='cuda:0') tensor(386839.5938, device='cuda:0') tensor(11883.9404, device='cuda:0') 0.03966077519109973\n",
      "Number of net: 0 tensor(1225693.2500, device='cuda:0') tensor(202696.8438, device='cuda:0') tensor(1022996.4375, device='cuda:0') 3.3678487446181693\n",
      "end of epoch:  4\n",
      "Number of net: 0\n",
      "tensor(45576312., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(217051.5312, device='cuda:0') tensor(206365.3594, device='cuda:0') tensor(10686.1650, device='cuda:0') 0.03590350531422969\n",
      "Number of net: 0 tensor(15980344., device='cuda:0') tensor(2910600.5000, device='cuda:0') tensor(13069743., device='cuda:0') 33.458979940381504\n",
      "Number of net: 0 tensor(33750460., device='cuda:0') tensor(8868249., device='cuda:0') tensor(24882212., device='cuda:0') 51.43923867450834\n",
      "Number of net: 0 tensor(368786.3438, device='cuda:0') tensor(361344., device='cuda:0') tensor(7442.3540, device='cuda:0') 0.025197793533962013\n",
      "Number of net: 0 tensor(13494413., device='cuda:0') tensor(3356084.2500, device='cuda:0') tensor(10138329., device='cuda:0') 27.35941158177775\n",
      "Number of net: 0 tensor(532567.5000, device='cuda:0') tensor(527902.2500, device='cuda:0') tensor(4665.2280, device='cuda:0') 0.01616436208050248\n",
      "Number of net: 0 tensor(3601690., device='cuda:0') tensor(474221.1875, device='cuda:0') tensor(3127468.7500, device='cuda:0') 10.052536118892043\n",
      "Number of net: 0 tensor(690888.4375, device='cuda:0') tensor(666571.6250, device='cuda:0') tensor(24316.8105, device='cuda:0') 0.08222454797075333\n",
      "Number of net: 0 tensor(990150.5625, device='cuda:0') tensor(163819.8906, device='cuda:0') tensor(826330.6875, device='cuda:0') 2.7658153179025815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of epoch:  5\n",
      "Number of net: 0\n",
      "tensor(21989424., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(288560.1875, device='cuda:0') tensor(91781.6172, device='cuda:0') tensor(196778.5781, device='cuda:0') 0.6931179937489332\n",
      "Number of net: 0 tensor(383588., device='cuda:0') tensor(237081.2969, device='cuda:0') tensor(146506.6875, device='cuda:0') 0.5292638079065602\n",
      "Number of net: 0 tensor(717916.8125, device='cuda:0') tensor(576660.4375, device='cuda:0') tensor(141256.3906, device='cuda:0') 0.514791949659259\n",
      "Number of net: 0 tensor(2103907., device='cuda:0') tensor(1102609.8750, device='cuda:0') tensor(1001297., device='cuda:0') 3.601693750742778\n",
      "Number of net: 0 tensor(497183.1875, device='cuda:0') tensor(435105.2812, device='cuda:0') tensor(62077.9062, device='cuda:0') 0.22823561125845612\n",
      "Number of net: 0 tensor(207177.9844, device='cuda:0') tensor(189898.6875, device='cuda:0') tensor(17279.3027, device='cuda:0') 0.0646409530718608\n",
      "Number of net: 0 tensor(897225., device='cuda:0') tensor(226461.1875, device='cuda:0') tensor(670763.8125, device='cuda:0') 2.4885284600957687\n",
      "Number of net: 0 tensor(1179422.7500, device='cuda:0') tensor(528813.1875, device='cuda:0') tensor(650609.6250, device='cuda:0') 2.3949335542452093\n",
      "Number of net: 0 tensor(649492., device='cuda:0') tensor(599349.0625, device='cuda:0') tensor(50142.9258, device='cuda:0') 0.1912719401112232\n",
      "end of epoch:  6\n",
      "Number of net: 0\n",
      "tensor(14988790., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(173689.2812, device='cuda:0') tensor(165586.3906, device='cuda:0') tensor(8102.8936, device='cuda:0') 0.03325187856231354\n",
      "Number of net: 0 tensor(305981.8750, device='cuda:0') tensor(301376.5000, device='cuda:0') tensor(4605.3613, device='cuda:0') 0.01930134603192518\n",
      "Number of net: 0 tensor(28314152., device='cuda:0') tensor(4650263., device='cuda:0') tensor(23663890., device='cuda:0') 59.558509504591285\n",
      "Number of net: 0 tensor(2088823.2500, device='cuda:0') tensor(408132.9375, device='cuda:0') tensor(1680690.2500, device='cuda:0') 6.945010997502108\n",
      "Number of net: 0 tensor(605328.7500, device='cuda:0') tensor(530615.3750, device='cuda:0') tensor(74713.3672, device='cuda:0') 0.332194111888087\n",
      "Number of net: 0 tensor(3844171.7500, device='cuda:0') tensor(155810.3906, device='cuda:0') tensor(3688361.2500, device='cuda:0') 15.079035242936524\n",
      "Number of net: 0 tensor(288880.5000, device='cuda:0') tensor(262969.2188, device='cuda:0') tensor(25911.2676, device='cuda:0') 0.12255120374056536\n",
      "Number of net: 0 tensor(486078.6250, device='cuda:0') tensor(477253.4062, device='cuda:0') tensor(8825.2207, device='cuda:0') 0.04323078990562662\n",
      "Number of net: 0 tensor(168574.9062, device='cuda:0') tensor(117333.0703, device='cuda:0') tensor(51241.8398, device='cuda:0') 0.26294409430284427\n",
      "end of epoch:  7\n",
      "Number of net: 0\n",
      "tensor(5795560.5000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(86197.0469, device='cuda:0') tensor(83059.3906, device='cuda:0') tensor(3137.6602, device='cuda:0') 0.016780160429217778\n",
      "Number of net: 0 tensor(547009.5000, device='cuda:0') tensor(208987.8906, device='cuda:0') tensor(338021.5938, device='cuda:0') 1.8328774981369806\n",
      "Number of net: 0 tensor(407690.5625, device='cuda:0') tensor(243661.6562, device='cuda:0') tensor(164028.8906, device='cuda:0') 0.9119528556465633\n",
      "Number of net: 0 tensor(342067.4375, device='cuda:0') tensor(331976.6250, device='cuda:0') tensor(10090.8252, device='cuda:0') 0.059388965359679344\n",
      "Number of net: 0 tensor(480864.2812, device='cuda:0') tensor(347552.0312, device='cuda:0') tensor(133312.2500, device='cuda:0') 0.7690588164532359\n",
      "Number of net: 0 tensor(253177.7500, device='cuda:0') tensor(141855.0156, device='cuda:0') tensor(111322.7422, device='cuda:0') 0.6543185876051804\n",
      "Number of net: 0 tensor(1337128., device='cuda:0') tensor(230648.3438, device='cuda:0') tensor(1106479.6250, device='cuda:0') 6.414880032606874\n",
      "Number of net: 0 tensor(1646011., device='cuda:0') tensor(437121.7500, device='cuda:0') tensor(1208889.2500, device='cuda:0') 7.091993897544871\n",
      "Number of net: 0 tensor(469295.7812, device='cuda:0') tensor(402008.5000, device='cuda:0') tensor(67287.2891, device='cuda:0') 0.44105855518802134\n",
      "end of epoch:  8\n",
      "Number of net: 0\n",
      "tensor(2389096., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(4367207., device='cuda:0') tensor(184592.9688, device='cuda:0') tensor(4182614., device='cuda:0') 25.403342894785148\n",
      "Number of net: 0 tensor(244973.7969, device='cuda:0') tensor(243453.5938, device='cuda:0') tensor(1520.1973, device='cuda:0') 0.011313209013152175\n",
      "Number of net: 0 tensor(423170.5625, device='cuda:0') tensor(373199.8438, device='cuda:0') tensor(49970.7188, device='cuda:0') 0.37848329894252536\n",
      "Number of net: 0 tensor(311343.5000, device='cuda:0') tensor(302624.1250, device='cuda:0') tensor(8719.3662, device='cuda:0') 0.07101212818407496\n",
      "Number of net: 0 tensor(172379.2500, device='cuda:0') tensor(116778.7188, device='cuda:0') tensor(55600.5352, device='cuda:0') 0.47772505594863846\n",
      "Number of net: 0 tensor(2431197.2500, device='cuda:0') tensor(202766.4219, device='cuda:0') tensor(2228430.7500, device='cuda:0') 18.428761100091823\n",
      "Number of net: 0 tensor(4706364., device='cuda:0') tensor(139138.4062, device='cuda:0') tensor(4567225.5000, device='cuda:0') 35.03885073222506\n",
      "Number of net: 0 tensor(234641.4375, device='cuda:0') tensor(233050.6875, device='cuda:0') tensor(1590.7526, device='cuda:0') 0.015340073274909926\n",
      "Number of net: 0 tensor(1104609.7500, device='cuda:0') tensor(152649.0781, device='cuda:0') tensor(951960.6875, device='cuda:0') 9.366042829433646\n",
      "end of epoch:  9\n",
      "Number of net: 0\n",
      "tensor(2839685.5000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(249865.4219, device='cuda:0') tensor(161766.1250, device='cuda:0') tensor(88099.2969, device='cuda:0') 0.9642862991930717\n",
      "Number of net: 0 tensor(2478794.2500, device='cuda:0') tensor(229831.9219, device='cuda:0') tensor(2248962.2500, device='cuda:0') 22.172801174403652\n",
      "Number of net: 0 tensor(274150.8125, device='cuda:0') tensor(273084.5312, device='cuda:0') tensor(1066.2909, device='cuda:0') 0.011682684261913795\n",
      "Number of net: 0 tensor(406106.8438, device='cuda:0') tensor(401469.9688, device='cuda:0') tensor(4636.8779, device='cuda:0') 0.05237172100219682\n",
      "Number of net: 0 tensor(283977.0625, device='cuda:0') tensor(280511.7500, device='cuda:0') tensor(3465.3025, device='cuda:0') 0.038798843882922054\n",
      "Number of net: 0 tensor(273066.2500, device='cuda:0') tensor(260438.4688, device='cuda:0') tensor(12627.7930, device='cuda:0') 0.13545264970146095\n",
      "Number of net: 0 tensor(16319268., device='cuda:0') tensor(1795570., device='cuda:0') tensor(14523698., device='cuda:0') 86.04103447201531\n",
      "Number of net: 0 tensor(1647010.1250, device='cuda:0') tensor(320557.4688, device='cuda:0') tensor(1326452.6250, device='cuda:0') 14.169662917197737\n",
      "Number of net: 0 tensor(10658892., device='cuda:0') tensor(1305021., device='cuda:0') tensor(9353871., device='cuda:0') 73.60470283090454\n",
      "end of epoch:  10\n",
      "Number of net: 0\n",
      "tensor(1368999.6250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(369972., device='cuda:0') tensor(182694.9688, device='cuda:0') tensor(187277.0312, device='cuda:0') 2.0948339605271724\n",
      "Number of net: 0 tensor(296770.5938, device='cuda:0') tensor(294496.3125, device='cuda:0') tensor(2274.2788, device='cuda:0') 0.024763676978203024\n",
      "Number of net: 0 tensor(217775.1562, device='cuda:0') tensor(146365.3594, device='cuda:0') tensor(71409.7891, device='cuda:0') 0.7799662215742352\n",
      "Number of net: 0 tensor(2514737.5000, device='cuda:0') tensor(593800.7500, device='cuda:0') tensor(1920936.6250, device='cuda:0') 20.176175540427245\n",
      "Number of net: 0 tensor(504971.5312, device='cuda:0') tensor(301171.7500, device='cuda:0') tensor(203799.7812, device='cuda:0') 2.290487614022824\n",
      "Number of net: 0 tensor(6944025.5000, device='cuda:0') tensor(1003768.3750, device='cuda:0') tensor(5940257., device='cuda:0') 55.62987731993737\n",
      "Number of net: 0 tensor(344032.3125, device='cuda:0') tensor(214230.8750, device='cuda:0') tensor(129801.4531, device='cuda:0') 1.378920521960318\n",
      "Number of net: 0 tensor(5564593., device='cuda:0') tensor(567806.6250, device='cuda:0') tensor(4996786.5000, device='cuda:0') 47.98438401560047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of net: 0 tensor(130104.0547, device='cuda:0') tensor(129125.5156, device='cuda:0') tensor(978.5408, device='cuda:0') 0.010129809479755092\n",
      "end of epoch:  11\n",
      "Number of net: 0\n",
      "tensor(5227215.5000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(214722.0781, device='cuda:0') tensor(191025.9219, device='cuda:0') tensor(23696.1621, device='cuda:0') 0.2358912291788127\n",
      "Number of net: 0 tensor(308990.5000, device='cuda:0') tensor(216964.4219, device='cuda:0') tensor(92026.0859, device='cuda:0') 0.8963489715966227\n",
      "Number of net: 0 tensor(415396.7188, device='cuda:0') tensor(344602.8750, device='cuda:0') tensor(70793.8359, device='cuda:0') 0.6786749471050655\n",
      "Number of net: 0 tensor(7440338.5000, device='cuda:0') tensor(1316947.1250, device='cuda:0') tensor(6123391.5000, device='cuda:0') 54.05978446372039\n",
      "Number of net: 0 tensor(879440.1875, device='cuda:0') tensor(244406.8906, device='cuda:0') tensor(635033.3125, device='cuda:0') 5.920278808293043\n",
      "Number of net: 0 tensor(327612.6250, device='cuda:0') tensor(318730.0938, device='cuda:0') tensor(8882.5400, device='cuda:0') 0.0776210602269968\n",
      "Number of net: 0 tensor(275777.2812, device='cuda:0') tensor(213798.0625, device='cuda:0') tensor(61979.2188, device='cuda:0') 0.5289145841183832\n",
      "Number of net: 0 tensor(616802.0625, device='cuda:0') tensor(512518.8438, device='cuda:0') tensor(104283.2188, device='cuda:0') 0.8672111085125265\n",
      "Number of net: 0 tensor(680942.3750, device='cuda:0') tensor(276833.5000, device='cuda:0') tensor(404108.8750, device='cuda:0') 3.3125604589481115\n",
      "end of epoch:  12\n",
      "Number of net: 0\n",
      "tensor(19175090., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(151072.6875, device='cuda:0') tensor(144115.4531, device='cuda:0') tensor(6957.2329, device='cuda:0') 0.05256027846106518\n",
      "Number of net: 0 tensor(1054764.5000, device='cuda:0') tensor(395583.3438, device='cuda:0') tensor(659181.1875, device='cuda:0') 4.9584757069281045\n",
      "Number of net: 0 tensor(1324755.8750, device='cuda:0') tensor(151038.6719, device='cuda:0') tensor(1173717.2500, device='cuda:0') 8.987932499115558\n",
      "Number of net: 0 tensor(1126624.5000, device='cuda:0') tensor(1106805.7500, device='cuda:0') tensor(19818.6934, device='cuda:0') 0.14622676869000012\n",
      "Number of net: 0 tensor(734416.6250, device='cuda:0') tensor(255913.2344, device='cuda:0') tensor(478503.4062, device='cuda:0') 3.4219313828769287\n",
      "Number of net: 0 tensor(4056093.7500, device='cuda:0') tensor(468468.4688, device='cuda:0') tensor(3587625.2500, device='cuda:0') 25.9876648741094\n",
      "Number of net: 0 tensor(211035.2656, device='cuda:0') tensor(209044.1875, device='cuda:0') tensor(1991.0796, device='cuda:0') 0.013103194623256462\n",
      "Number of net: 0 tensor(7402466., device='cuda:0') tensor(656408.8750, device='cuda:0') tensor(6746057., device='cuda:0') 45.11861176731132\n",
      "Number of net: 0 tensor(1.7604e+08, device='cuda:0') tensor(1.4129e+08, device='cuda:0') tensor(34750776., device='cuda:0') 72.66641943736434\n",
      "end of epoch:  13\n",
      "Number of net: 0\n",
      "tensor(2.1166e+08, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(931654.5000, device='cuda:0') tensor(450869.2812, device='cuda:0') tensor(480785.2188, device='cuda:0') 3.169999739284099\n",
      "Number of net: 0 tensor(5.4577e+08, device='cuda:0') tensor(2.7469e+08, device='cuda:0') tensor(2.7108e+08, device='cuda:0') 80.25750307947365\n",
      "Number of net: 0 tensor(287449.2500, device='cuda:0') tensor(275032.6562, device='cuda:0') tensor(12416.5957, device='cuda:0') 0.0762379199950038\n",
      "Number of net: 0 tensor(1.4930e+09, device='cuda:0') tensor(7.2247e+08, device='cuda:0') tensor(7.7056e+08, device='cuda:0') 84.13594944592778\n",
      "Number of net: 0 tensor(6715693.5000, device='cuda:0') tensor(1105452.1250, device='cuda:0') tensor(5610241.5000, device='cuda:0') 34.78306884244519\n",
      "Number of net: 0 tensor(861236.8750, device='cuda:0') tensor(494278.5625, device='cuda:0') tensor(366958.3125, device='cuda:0') 2.0240815762523674\n",
      "Number of net: 0 tensor(316058.5000, device='cuda:0') tensor(312135.3125, device='cuda:0') tensor(3923.2002, device='cuda:0') 0.02126302158386332\n",
      "Number of net: 0 tensor(465087.5000, device='cuda:0') tensor(456336.3750, device='cuda:0') tensor(8751.1338, device='cuda:0') 0.04930642155011943\n",
      "Number of net: 0 tensor(6464821., device='cuda:0') tensor(944823.3125, device='cuda:0') tensor(5519997.5000, device='cuda:0') 34.62384872820833\n",
      "end of epoch:  14\n",
      "Number of net: 0\n",
      "tensor(7.6230e+08, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(50400.5078, device='cuda:0') tensor(47547.9141, device='cuda:0') tensor(2852.5923, device='cuda:0') 0.016241483327875617\n",
      "Number of net: 0 tensor(312086.7188, device='cuda:0') tensor(231481.0156, device='cuda:0') tensor(80605.7109, device='cuda:0') 0.46832938595089696\n",
      "Number of net: 0 tensor(783314.0625, device='cuda:0') tensor(395271.1562, device='cuda:0') tensor(388042.9062, device='cuda:0') 2.2801331863943095\n",
      "Number of net: 0 tensor(4395958.5000, device='cuda:0') tensor(815007., device='cuda:0') tensor(3580951.5000, device='cuda:0') 22.556921596101663\n",
      "Number of net: 0 tensor(580838.5000, device='cuda:0') tensor(539877.3125, device='cuda:0') tensor(40961.1680, device='cuda:0') 0.23077462311706834\n",
      "Number of net: 0 tensor(7538754., device='cuda:0') tensor(1046510.9375, device='cuda:0') tensor(6492243., device='cuda:0') 37.648395227554104\n",
      "Number of net: 0 tensor(6162454.5000, device='cuda:0') tensor(1646430.3750, device='cuda:0') tensor(4516024., device='cuda:0') 27.064884372613776\n",
      "Number of net: 0 tensor(1188798.1250, device='cuda:0') tensor(1171508.7500, device='cuda:0') tensor(17289.3730, device='cuda:0') 0.0901673782906704\n",
      "Number of net: 0 tensor(976974.3750, device='cuda:0') tensor(284920.8125, device='cuda:0') tensor(692053.5625, device='cuda:0') 3.8011823689035666\n",
      "end of epoch:  15\n",
      "Number of net: 0\n",
      "tensor(1.9975e+11, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(5660305., device='cuda:0') tensor(614812.7500, device='cuda:0') tensor(5045492., device='cuda:0') 30.275121082067994\n",
      "Number of net: 0 tensor(10136032., device='cuda:0') tensor(2642156.5000, device='cuda:0') tensor(7493876., device='cuda:0') 41.120890823998096\n",
      "Number of net: 0 tensor(583020.6250, device='cuda:0') tensor(577451.9375, device='cuda:0') tensor(5568.7026, device='cuda:0') 0.030046683915102675\n",
      "Number of net: 0 tensor(383294.5938, device='cuda:0') tensor(281149.2812, device='cuda:0') tensor(102145.3203, device='cuda:0') 0.5434053740721266\n",
      "Number of net: 0 tensor(143685.3438, device='cuda:0') tensor(140783.3906, device='cuda:0') tensor(2901.9473, device='cuda:0') 0.014785422976562579\n",
      "Number of net: 0 tensor(653334.9375, device='cuda:0') tensor(170556.7969, device='cuda:0') tensor(482778.1250, device='cuda:0') 2.4780536681424934\n",
      "Number of net: 0 tensor(6144634., device='cuda:0') tensor(994852.7500, device='cuda:0') tensor(5149781., device='cuda:0') 27.69064716605034\n",
      "Number of net: 0 tensor(5762555.5000, device='cuda:0') tensor(636154.6250, device='cuda:0') tensor(5126401., device='cuda:0') 27.28834631151617\n",
      "Number of net: 0 tensor(1204511.5000, device='cuda:0') tensor(672156.8125, device='cuda:0') tensor(532354.6875, device='cuda:0') 2.6260933000586593\n",
      "end of epoch:  16\n",
      "Number of net: 0\n",
      "tensor(9.0538e+11, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(3921019.5000, device='cuda:0') tensor(799794.1875, device='cuda:0') tensor(3121225.2500, device='cuda:0') 16.41606887927903\n",
      "Number of net: 0 tensor(753544.1875, device='cuda:0') tensor(222153.6250, device='cuda:0') tensor(531390.5625, device='cuda:0') 2.689019372749047\n",
      "Number of net: 0 tensor(6556141., device='cuda:0') tensor(995687.6875, device='cuda:0') tensor(5560453.5000, device='cuda:0') 26.832079630081978\n",
      "Number of net: 0 tensor(837919.1250, device='cuda:0') tensor(693893.2500, device='cuda:0') tensor(144025.8750, device='cuda:0') 0.6884137352939284\n",
      "Number of net: 0 tensor(3.7238e+08, device='cuda:0') tensor(93349280., device='cuda:0') tensor(2.7903e+08, device='cuda:0') 70.87393291684779\n",
      "Number of net: 0 tensor(418838.5000, device='cuda:0') tensor(375154.2812, device='cuda:0') tensor(43684.2227, device='cuda:0') 0.20255958297136212\n",
      "Number of net: 0 tensor(1.4298e+12, device='cuda:0') tensor(6.9585e+10, device='cuda:0') tensor(1.3602e+12, device='cuda:0') 92.19766036876885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of net: 0 tensor(9967566., device='cuda:0') tensor(1207787.5000, device='cuda:0') tensor(8759778., device='cuda:0') 36.918974185218964\n",
      "Number of net: 0 tensor(3588469.2500, device='cuda:0') tensor(423227.6875, device='cuda:0') tensor(3165241.5000, device='cuda:0') 14.502772196819356\n",
      "end of epoch:  17\n",
      "Number of net: 0\n",
      "tensor(2.8746e+10, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(1958797.7500, device='cuda:0') tensor(468243.2500, device='cuda:0') tensor(1490554.5000, device='cuda:0') 6.906385171824923\n",
      "Number of net: 0 tensor(1.5816e+09, device='cuda:0') tensor(6.4318e+08, device='cuda:0') tensor(9.3846e+08, device='cuda:0') 81.81031012831652\n",
      "Number of net: 0 tensor(1545692.5000, device='cuda:0') tensor(477463.0625, device='cuda:0') tensor(1068229.3750, device='cuda:0') 4.79551098126083\n",
      "Number of net: 0 tensor(683448.6875, device='cuda:0') tensor(634358.8125, device='cuda:0') tensor(49089.8633, device='cuda:0') 0.21359321264298078\n",
      "Number of net: 0 tensor(348279.0625, device='cuda:0') tensor(168001.5625, device='cuda:0') tensor(180277.4844, device='cuda:0') 0.7835227238501256\n",
      "Number of net: 0 tensor(310710.0625, device='cuda:0') tensor(306138.6250, device='cuda:0') tensor(4571.4292, device='cuda:0') 0.01932593034523069\n",
      "Number of net: 0 tensor(197282.8125, device='cuda:0') tensor(194819.6406, device='cuda:0') tensor(2463.1758, device='cuda:0') 0.010313695582839035\n",
      "Number of net: 0 tensor(435682.5625, device='cuda:0') tensor(384960.0312, device='cuda:0') tensor(50722.5312, device='cuda:0') 0.20599303059355026\n",
      "Number of net: 0 tensor(402242.0625, device='cuda:0') tensor(391289.5312, device='cuda:0') tensor(10952.5332, device='cuda:0') 0.04288113669413419\n",
      "end of epoch:  18\n",
      "Number of net: 0\n",
      "tensor(8.8177e+12, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(226028.6719, device='cuda:0') tensor(221641.3906, device='cuda:0') tensor(4387.2808, device='cuda:0') 0.016489134846952313\n"
     ]
    }
   ],
   "source": [
    "loss_fn_nets =[]\n",
    "nets = []\n",
    "optimizer_nets = []\n",
    "mu_glob = []\n",
    "sigma_glob = []\n",
    "prune_coef_glob = []\n",
    "init_nets_output =  init_nets(loss_fn_nets)\n",
    "train_nets(init_nets_output[0], init_nets_output[1], init_nets_output[2], init_nets_output[3], init_nets_output[4])\n",
    "old_nets = nets[:]\n",
    "\n",
    "for k,lam in enumerate(lamb):\n",
    "    for i in range(len(nets)):\n",
    "        new_net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "        for j in range(0, 2): # бежим по слоям\n",
    "            lam_param = lam / 10**log_lam_high\n",
    "            new_net[j].mean.data*=0\n",
    "            new_net[j].mean.data+=old_nets[i][j].mean(lam_param)\n",
    "            new_net[j].mean_b.data*=0\n",
    "            new_net[j].mean_b.data+=old_nets[i][j].mean_b(lam_param)\n",
    "            new_net[j].log_sigma.data*=0\n",
    "            new_net[j].log_sigma.data+=old_nets[i][j].log_sigma(lam_param)\n",
    "            new_net[j].log_sigma_b.data*=0\n",
    "            new_net[j].log_sigma_b.data+=old_nets[i][j].log_sigma_b(lam_param)\n",
    "            \n",
    "    nets[i] = new_net\n",
    "            \n",
    "\n",
    "        \n",
    "    acc_delete.append(None)\n",
    "    init_coeff(prune_coef_glob, mu_glob, sigma_glob)\n",
    "    acc_delete[k]= delete_10(acc_delete[k], prune_coef_glob, mu_glob, sigma_glob, nets, init_nets_output[0])\n",
    "\n",
    "    \n",
    "init_coeff(prune_coef_glob, mu_glob, sigma_glob)    \n",
    "#graph_loss_func()\n",
    "graph(acc_delete,lamb)\n",
    "save(acc_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legin/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "new_net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "i = 0 \n",
    "for j in range(0, 2): # бежим по слоям\n",
    "    new_net[j].mean.data*=0\n",
    "    new_net[j].mean.data+=old_nets[i][j].mean(lam)\n",
    "    new_net[j].mean_b.data*=0\n",
    "    new_net[j].mean_b.data+=old_nets[i][j].mean_b(lam)\n",
    "    new_net[j].log_sigma.data*=0\n",
    "    new_net[j].log_sigma.data+=old_nets[i][j].log_sigma(lam)\n",
    "    new_net[j].log_sigma_b.data*=0\n",
    "    new_net[j].log_sigma_b.data+=old_nets[i][j].log_sigma_b(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1213.8149,  885.8934, 1215.5417,  883.6002,  768.9736, 1339.3053,\n",
       "         1235.9641, 1001.3223, 1015.4268, 1144.4662],\n",
       "        [1113.5548,  825.9999, 1122.1970,  829.0692,  703.5684, 1222.4860,\n",
       "         1139.6187,  909.0403,  941.6868, 1039.6541],\n",
       "        [1339.7222, 1000.9510, 1352.3000,  987.5376,  854.2031, 1484.1738,\n",
       "         1375.6842, 1108.0099, 1130.4620, 1266.8187],\n",
       "        [1002.4268,  716.7179,  990.4283,  723.5088,  620.0521, 1093.7192,\n",
       "         1012.1857,  808.2672,  829.2755,  925.5312],\n",
       "        [1214.7711,  881.5259, 1217.4650,  878.8027,  773.8610, 1341.7554,\n",
       "         1239.9010,  995.3021, 1017.7711, 1147.7567],\n",
       "        [1264.0046,  948.8234, 1279.3776,  936.2783,  807.3743, 1401.2827,\n",
       "         1299.3010, 1048.4663, 1071.0332, 1198.0283],\n",
       "        [1162.2483,  848.7446, 1162.3492,  849.2804,  742.9947, 1287.5316,\n",
       "         1187.1021,  956.5825,  976.6001, 1098.8002],\n",
       "        [1161.8960,  854.9814, 1167.5698,  849.1383,  741.3011, 1291.7340,\n",
       "         1193.3240,  957.9014,  978.7094, 1101.9408],\n",
       "        [1055.0309,  765.3071, 1059.2241,  759.6130,  665.5242, 1162.6490,\n",
       "         1075.7008,  858.5210,  879.9997,  991.1271],\n",
       "        [1004.6137,  726.8611, 1002.0803,  724.5376,  640.0945, 1113.4681,\n",
       "         1027.1365,  826.8737,  842.0551,  953.8392],\n",
       "        [1106.1322,  800.0879, 1096.8038,  803.9124,  687.3207, 1209.8391,\n",
       "         1110.4406,  893.7808,  922.4234, 1020.5682],\n",
       "        [1102.7822,  812.9637, 1107.4323,  809.6740,  701.6365, 1216.2111,\n",
       "         1128.6803,  904.2400,  930.2513, 1036.0732],\n",
       "        [1142.2748,  827.7216, 1142.2802,  825.7719,  727.3287, 1263.5828,\n",
       "         1167.0024,  938.4810,  956.5614, 1082.3589],\n",
       "        [1114.2269,  798.5081, 1101.5996,  802.3863,  690.2389, 1217.2181,\n",
       "         1117.6390,  899.4980,  925.4005, 1032.5500],\n",
       "        [1233.8114,  925.1736, 1247.6390,  913.9560,  785.8114, 1366.6178,\n",
       "         1268.7147, 1020.9987, 1044.6176, 1168.4497],\n",
       "        [1183.5101,  869.5018, 1183.6874,  868.3387,  748.7723, 1305.7465,\n",
       "         1203.5662,  968.9479,  995.5602, 1110.4602],\n",
       "        [1151.9600,  832.7865, 1152.1295,  831.0288,  731.0743, 1272.2629,\n",
       "         1175.2589,  943.7976,  962.8796, 1090.2181],\n",
       "        [1156.8367,  843.4811, 1156.2766,  844.3018,  730.8408, 1274.0698,\n",
       "         1175.9003,  952.3433,  965.7720, 1087.3026],\n",
       "        [ 959.4685,  712.2230,  965.9986,  715.6331,  614.0234, 1060.5006,\n",
       "          986.8311,  790.3065,  813.6065,  907.2649],\n",
       "        [1238.1980,  900.6582, 1239.1887,  898.3592,  790.9783, 1372.5751,\n",
       "         1266.3883, 1016.5413, 1036.9979, 1170.7600],\n",
       "        [1055.3127,  769.6971, 1053.7079,  772.3586,  671.9940, 1168.5079,\n",
       "         1077.4681,  872.6092,  886.8295, 1000.7932],\n",
       "        [1187.3479,  870.8310, 1190.4342,  873.1470,  758.6207, 1314.7000,\n",
       "         1221.4469,  968.9639, 1000.5348, 1116.7037],\n",
       "        [1133.0822,  839.0484, 1142.8619,  830.1492,  727.1671, 1254.1469,\n",
       "         1167.9001,  936.0645,  955.9817, 1072.5751],\n",
       "        [1206.0485,  878.1080, 1204.0670,  879.5761,  762.6722, 1333.1544,\n",
       "         1228.8014,  983.7407, 1009.8130, 1130.5684],\n",
       "        [1291.6489,  943.6645, 1295.6732,  938.1758,  823.0602, 1430.2521,\n",
       "         1321.5273, 1063.3856, 1081.4053, 1221.7062],\n",
       "        [ 775.9562,  540.8037,  758.5713,  550.2057,  476.9268,  839.6254,\n",
       "          774.0443,  616.4178,  637.5400,  703.3408],\n",
       "        [1262.4897,  919.6780, 1261.5057,  916.8309,  797.2607, 1393.0698,\n",
       "         1283.7463, 1038.5009, 1051.4291, 1189.8137],\n",
       "        [1162.3121,  842.5047, 1163.0836,  842.0964,  742.7407, 1286.8655,\n",
       "         1187.2418,  951.1564,  975.0425, 1097.7513],\n",
       "        [1033.9384,  742.7003, 1023.7916,  753.6508,  639.1855, 1126.3632,\n",
       "         1036.4337,  831.7239,  860.6248,  953.3066],\n",
       "        [1303.6235,  970.5648, 1311.9340,  960.6790,  829.9347, 1443.7478,\n",
       "         1336.0490, 1074.9747, 1099.9890, 1230.5431],\n",
       "        [1112.7043,  821.3407, 1112.8785,  825.1525,  704.0001, 1231.5591,\n",
       "         1137.2792,  914.7655,  935.8028, 1048.7450],\n",
       "        [1329.9685,  985.8568, 1335.3694,  975.9597,  844.0504, 1471.8356,\n",
       "         1360.0922, 1096.3303, 1117.1455, 1254.8982],\n",
       "        [1094.7808,  808.6679, 1095.6768,  812.6392,  695.2275, 1212.7867,\n",
       "         1118.1213,  895.7611,  924.9702, 1029.1608],\n",
       "        [1181.4437,  853.0333, 1174.9203,  847.8520,  744.7269, 1297.0996,\n",
       "         1200.7886,  960.3172,  983.4108, 1101.1309],\n",
       "        [1089.5023,  796.8277, 1096.9160,  793.4803,  689.8644, 1204.1743,\n",
       "         1108.9688,  897.8516,  915.6964, 1030.6798],\n",
       "        [1092.6499,  802.0144, 1104.6135,  800.3990,  690.2056, 1204.0435,\n",
       "         1115.1886,  892.2275,  919.6136, 1022.9302],\n",
       "        [1171.6873,  852.2813, 1174.0078,  851.6686,  737.9956, 1292.9839,\n",
       "         1192.7706,  964.4850,  976.4904, 1104.4377],\n",
       "        [1279.8199,  955.3799, 1289.7539,  942.8917,  814.4152, 1417.7841,\n",
       "         1312.1676, 1057.3673, 1080.2207, 1209.7961],\n",
       "        [1245.4636,  924.5058, 1254.9376,  922.3326,  785.9587, 1372.2797,\n",
       "         1271.9065, 1019.3018, 1050.3654, 1166.7529],\n",
       "        [1205.7753,  906.9663, 1221.0624,  896.1619,  769.2404, 1335.8564,\n",
       "         1240.4474,  997.6205, 1024.6687, 1143.0664],\n",
       "        [1394.8994, 1035.0139, 1403.2163, 1022.6022,  885.2772, 1543.6072,\n",
       "         1428.2947, 1149.7947, 1171.8206, 1316.7170],\n",
       "        [1234.2463,  900.1197, 1237.5350,  896.0052,  780.3107, 1364.4656,\n",
       "         1259.8972, 1017.8138, 1030.8387, 1165.6505],\n",
       "        [1088.5558,  799.7700, 1097.3922,  795.2089,  701.5529, 1210.6924,\n",
       "         1118.9413,  901.7348,  919.4437, 1038.4702],\n",
       "        [1260.8257,  939.2440, 1276.6754,  929.8670,  806.9221, 1393.5586,\n",
       "         1294.6140, 1039.9116, 1068.7317, 1193.2734],\n",
       "        [1215.5618,  899.1376, 1219.9398,  899.6274,  772.4246, 1344.3591,\n",
       "         1246.4971,  998.8228, 1023.1666, 1143.8209],\n",
       "        [1214.5325,  888.2198, 1212.3229,  893.5019,  767.9814, 1340.2347,\n",
       "         1235.2024,  989.7374, 1019.8769, 1135.8445],\n",
       "        [1194.7740,  888.1383, 1203.9762,  882.1531,  761.4463, 1325.8440,\n",
       "         1225.4667,  985.2586, 1010.0103, 1129.3053],\n",
       "        [1256.6650,  925.7350, 1270.0209,  918.4496,  801.4917, 1391.3363,\n",
       "         1290.8925, 1034.4359, 1057.5919, 1188.2239],\n",
       "        [ 882.3761,  634.4458,  879.8942,  641.2602,  568.7238,  982.0277,\n",
       "          902.1945,  722.9243,  745.1243,  837.5640],\n",
       "        [1119.4977,  819.4255, 1128.1768,  816.5215,  720.9461, 1240.3503,\n",
       "         1151.3230,  923.1666,  944.4438, 1063.2751],\n",
       "        [1230.1693,  900.2593, 1232.3743,  902.9976,  781.8223, 1357.0557,\n",
       "         1264.3557, 1005.2056, 1031.3872, 1154.1401],\n",
       "        [ 923.4564,  678.5153,  926.4617,  686.3658,  583.9376, 1020.4092,\n",
       "          943.3774,  757.4249,  778.7120,  870.8770],\n",
       "        [1148.5693,  830.2279, 1140.0096,  829.3983,  723.0687, 1268.8618,\n",
       "         1165.0298,  935.1100,  954.9310, 1073.4139],\n",
       "        [1265.6339,  929.6724, 1266.3108,  928.9948,  802.3307, 1399.1946,\n",
       "         1290.1858, 1036.9296, 1062.9387, 1189.9510],\n",
       "        [ 984.7880,  726.6226,  992.9525,  723.3234,  632.6845, 1087.6653,\n",
       "         1014.3206,  811.7478,  833.4928,  930.4764],\n",
       "        [1136.6107,  821.8636, 1129.2489,  830.7013,  713.0267, 1245.5730,\n",
       "         1148.6978,  919.4561,  953.3442, 1054.4016],\n",
       "        [1065.7957,  772.9418, 1067.4376,  772.8953,  684.2037, 1180.3480,\n",
       "         1092.5101,  872.3882,  895.6629, 1007.6088],\n",
       "        [1328.8558,  992.5547, 1341.7382,  979.4781,  844.9699, 1471.5966,\n",
       "         1362.9536, 1098.3242, 1121.3163, 1257.1777],\n",
       "        [1088.2820,  785.3709, 1084.9475,  783.9604,  691.8876, 1202.8215,\n",
       "         1110.1906,  892.6973,  908.5729, 1030.7645],\n",
       "        [1330.9313,  977.9062, 1329.9781,  970.1743,  843.6094, 1469.2367,\n",
       "         1356.8473, 1092.7114, 1113.6824, 1248.8262],\n",
       "        [1101.1812,  809.7220, 1105.1852,  809.8719,  699.6218, 1218.2960,\n",
       "         1125.3260,  912.6066,  923.8100, 1040.3481],\n",
       "        [1035.4622,  753.7729, 1041.9268,  751.0491,  654.8101, 1138.8579,\n",
       "         1056.0887,  844.3660,  872.7928,  973.6345],\n",
       "        [1292.9025,  942.0190, 1294.7795,  936.4296,  819.6076, 1428.9954,\n",
       "         1319.5942, 1059.7574, 1081.0751, 1219.5033],\n",
       "        [1163.7986,  859.3628, 1176.5050,  859.6938,  741.1757, 1287.1127,\n",
       "         1192.1193,  956.1595,  983.4633, 1099.9132]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
